#!/usr/bin/python3
import argparse
import os
import traceback
from datetime import date
from datetime import datetime
from datetime import timedelta
from pathlib import Path

from ocflib.lab.stats import get_connection
from ocflib.lab.stats import humanize_bytes

OCFSTATS_PWD = os.environ.get('OCFSTATS_PWD', None)
MIRRORS_DATA_PATH = Path('/opt/mirrors/ftp')
APACHE_LOG_PATH = Path('/var/log/apache2')
APACHE_LOG_FILES = [
    Path('mirrors.ocf.berkeley.edu_access.log'),
    Path('mirrors.ocf.berkeley.edu-ssl_access_ssl.log')
]
RSYNC_LOG_PATH = Path('/var/log/rsync')


def log_to_mysql(projects, dt=None, quiet=False):
    log_date = dt or date.today()

    with get_connection(user='ocfstats', password=OCFSTATS_PWD) as cursor:
        for name, data in projects.items():
            cursor.execute(
                'INSERT INTO `mirrors` (`date`, `dist`, `up`, `down`) '
                'VALUES (%s, %s, %s, %s)',
                (log_date, name, data['up'], data['down'])
            )


def process_apache_log(projects, fn, log_date):
    """Extract transfer data for a given day for all projects from an Apache log file.

    We iteratively populate projects from all the log files in
    consideration (4 by default) to avoid reading the entire file
    repeatedly for multiple projects and to avoid having to merge
    dictionaries.
    """

    with fn.open('r') as f:
        for n, line in enumerate(f, 1):
            stats = line.split()

            # apache log date format looks like [11/Jul/2017:00:05:16 -0700]
            # line.split()[3][1:12] = 11/Jul/2017
            # we need to dump dates that don't match because
            # logrotate rotates the logs at 6am
            try:
                line_date = datetime.strptime(stats[3][1:12], '%d/%b/%Y')
            except ValueError:
                print('Error on line {}:\\  {}\\'.format(n, line))
                traceback.print_exc()

            if line_date.date() != log_date:
                continue

            # extract dist name from request url
            # '/debian/pool/main/h/hwdata/...' -> 'debian'
            dist = stats[6]
            dist = dist.split('/')[1] if '/' in dist else dist

            # record if we returned http 2xx/3xx
            if stats[8][0] in ('2', '3') and dist in projects:
                projects[dist]['up'] += int(stats[-2])
                projects[dist]['down'] += int(stats[-1])
            else:
                projects['other']['up'] += int(stats[-2])
                projects['other']['down'] += int(stats[-1])

    return projects


def process_rsync_log(filename):
    """Parse Tx/Rx information out of an rsyncd log file.

    This function is project-agnostic.
    """

    tx = 0
    rx = 0
    with filename.open('r') as f:
        for line in f.readlines():
            # log format is <datetime> <pid> sent <tx> bytes received <rx> bytes total size
            if line.find('sent') > 0:
                stats = line.split()
                tx += int(stats[4])
                rx += int(stats[7])

    return tx, rx


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Process mirrors logs to calculate network usage '
                                                 'and store in ocfstats')
    parser.add_argument('-q', '--quiet', action='store_true',
                        help='do not print stats after collecting them')
    parser.add_argument('-n', '--dry-run', action='store_true',
                        help='dry-run mode, do not write statistics to the database')
    parser.add_argument('--apache', nargs='*',
                        help='Apache log file(s) to process')
    parser.add_argument('--rsync', nargs='*',
                        help='rsync log files(s) to process')
    parser.add_argument('date', nargs='?', default=date.today() - timedelta(1),
                        help='date for use in filtering log entries')

    args = parser.parse_args()

    # first, figure out what projects we're getting stats for
    sources = []
    for project in MIRRORS_DATA_PATH.iterdir():
        if project.is_dir() and not project.name.startswith('.'):
            sources.append(project.name)

    sources.append('other')  # catchall

    # then, figure out which Apache logs we're pulling stats from
    if args.apache:
        apache_log_files = args.apache
    else:
        apache_log_files = []
        for log_name in APACHE_LOG_FILES:
            # logrotate rotates the log at 6am, but this script
            # runs at midnight. So to capture 24h of data, we need
            # to parse the rotated log as well. (the .1 file)
            log = APACHE_LOG_PATH / log_name
            apache_log_files.append(log)
            apache_log_files.append(Path(str(log) + '.1'))

    # and the same for rsync
    if args.rsync:
        rsync_log_files = args.rsync
    else:
        rsync_log_files = []
        for log_file in RSYNC_LOG_PATH.iterdir():
            # log files are named <project>.log
            rsync_log_files.append(log_file)

    # set up data structures
    projects = {}
    for project in sources:
        projects[project] = {'up': 0, 'down': 0}

    # pull the data itself from apache
    for log_filename in apache_log_files:
        projects = process_apache_log(projects, log_filename, args.date)

    # ...and rsync
    for log_filename in rsync_log_files:
        tx, rx = process_rsync_log(log_filename)
        projects[log_filename.stem]['up'] += rx
        projects[log_filename.stem]['down'] += tx

    # print out some stats!
    if not args.quiet:
        for project in projects:
            print('{:20} {:10} {:10}'.format(
                project,
                humanize_bytes(projects[project]['up']),
                humanize_bytes(projects[project]['down'])
            ))

    # and send it to mysql
    if not args.dry_run:
        log_to_mysql(projects=projects, dt=args.date)
